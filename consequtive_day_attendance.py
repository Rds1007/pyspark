# -*- coding: utf-8 -*-
"""Consequtive_day_attendance

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16SFGz21HT6w_hki6n2uCd3fJXdgm_rEZ
"""

from datetime import date
from pyspark.sql import SparkSession
from pyspark.sql.functions import col,lag,lead,datediff,to_date,row_number,day,max,min,count,lit
from pyspark.sql.types import DateType
from pyspark.sql.window import Window
spark = SparkSession.builder.appName('test').getOrCreate()

_data = [(101,'02-01-2024','N'),
(101,'03-01-2024','Y'),
(101,'04-01-2024','N'),
(101,'07-01-2024','Y'),
(102,'01-01-2024','N'),
(102,'02-01-2024','Y'),
(102,'03-01-2024','Y'),
(102,'04-01-2024','N'),
(102,'05-01-2024','Y'),
(102,'06-01-2024','Y'),
(102,'07-01-2024','Y'),
(103,'01-01-2024','N'),
(103,'04-01-2024','N'),
(103,'05-01-2024','Y'),
(103,'06-01-2024','Y'),
(103,'07-01-2024','N')
]
_schema = ["emp_id" , "log_date" , "flag"]

df = spark.createDataFrame(data = _data , schema = _schema)
print("Printing the input")
df.show()
print("Get the employees who attended 2 or more consequtiuve days")
df=df.filter(col("flag")=='Y')

windowspec=Window.partitionBy("emp_id").orderBy(col("log_date").asc())

df_part=df.withColumn("row_num",row_number().over(windowspec))
df_part=df_part.withColumn("log_date",to_date(col("log_date"),"dd-MM-yyyy"))
df_part=df_part.withColumn("dayN",day(col("log_date")))
df_part=df_part.withColumn("diffN",col("dayN")-col("row_num"))
df_final=df_part.groupBy("emp_id","diffN").agg(min("log_date").alias("start_date"),max("log_date").alias("end_date"),count(lit(1)).alias("count"))
df_final=df_final.withColumn("start_date",to_date(col("start_date"),"dd-MM-yyyy"))
df_final=df_final.drop("diffN")
print("Printing the output")
df_final.select("emp_id","start_date","end_date","count").filter(col("count")>1).show()