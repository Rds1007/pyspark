# -*- coding: utf-8 -*-
"""Analytical_functions_pyspark

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MZbAmfGTVanQIyw_v9KcG23pimDzDxoR
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import row_number,sum,rank,dense_rank,percent_rank,ntile,lead,lag
from pyspark.sql.window import Window
from pyspark.sql.types import StructType, StructField, IntegerType, StringType


spark = SparkSession.builder.appName("SparkByExamples.com").getOrCreate()


emp_data = [
    (101, "Varun", "Sales", 75000),
    (102, "Alia", "HR", 46000),
    (103, "David", "IT", 55000),
    (104, "Steve", "Sales", 75000),
    (105, "Soham", "HR", 46000),
    (106, "Kiron", "IT", 50000),
    (107, "Dhoni", "Sales", 68000),
    (108, "Tiger", "HR", 45000),
    (109, "Rock", "IT", 53000),
    (110, "Khali", "Sales", 75000)
]


emp_schema = StructType([
    StructField("EmployeeID", IntegerType(), True),
    StructField("Name", StringType(), True),
    StructField("Department", StringType(), True),
    StructField("Salary", IntegerType(), True)
])



emp_data = spark.createDataFrame(data = emp_data , schema = emp_schema)
Windowspec=Window.partitionBy(emp_data["Department"]).orderBy(emp_data["Salary"].desc())

emp_sorted=emp_data.withColumn("Rank",row_number().over(Windowspec))

emp_sorted.show()



Windowspec=Window.partitionBy(emp_data["Department"]).orderBy(emp_data["Salary"].desc())

emp_rank=emp_data.withColumn("rank",rank().over(Windowspec))

emp_rank.show()

Windowspec=Window.partitionBy(emp_data["Department"]).orderBy(emp_data["Salary"].desc())

emp_rank=emp_data.withColumn("rank",dense_rank().over(Windowspec))

emp_rank.show()

Windowspec=Window.partitionBy(emp_data["Department"]).orderBy(emp_data["Salary"].desc())

emp_rank=emp_data.withColumn("prv_sal",lag("salary",1).over(Windowspec))

emp_rank.show()

Windowspec=Window.partitionBy(emp_data["Department"]).orderBy(emp_data["Salary"].desc())

emp_rank=emp_data.withColumn("prv_sal",lead("salary",1).over(Windowspec))

emp_rank.show()

##Running total within partition
Windowspec=Window.partitionBy(emp_data["Department"]).orderBy(emp_data["Salary"].desc()).rowsBetween(-1, 0)

emp_rank=emp_data.withColumn("runingtotal",sum("salary").over(Windowspec))

emp_rank.show()

from pyspark.sql import SparkSession
from pyspark.sql.functions import avg
from pyspark.sql.window import Window
from pyspark.sql.types import StructType, StructField, DateType, IntegerType
from datetime import date

spark = SparkSession.builder.getOrCreate()


data = [
    (date(2024, 4, 1), 100),
    (date(2024, 4, 2), 120),
    (date(2024, 4, 3), 130),
    (date(2024, 4, 4), 140),
    (date(2024, 4, 5), 160),
]

schema = StructType([
    StructField("date", DateType(), True),
    StructField("value", IntegerType(), True),
])

df = spark.createDataFrame(data, schema)

##7 Days Moving Averages

from pyspark.sql.functions import avg, col,round
from pyspark.sql.window import Window
from pyspark.sql.functions import unix_timestamp

# Add timestamp column
df = df.withColumn("timestamp", unix_timestamp("date").cast("long"))

# Define window: 6 days before (in seconds) to today
# 6 days * 86400 seconds/day = 518400 seconds
windowSpec = Window.orderBy("timestamp").rangeBetween(-518400, 0)

# Compute 7-day moving average
df_with_avg = df.withColumn("7_day_avg", avg("value").over(windowSpec))
df_with_avg.select("date", "value", "7_day_avg").show()

df_with_avg=df_with_avg.withColumn("7_day_avg",round("7_day_avg",2))
df_with_avg.show()